{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bfcda6",
   "metadata": {},
   "source": [
    "## Simple GPT2 Impelmentation\n",
    "\n",
    "This notebook implements a simple GPT2 model based on the archicture from: \n",
    "    \n",
    "Improving Language Understanding by Generative Pre-Training (OpenAI)\n",
    "\n",
    "Also useful for tranformers: \n",
    "\n",
    "Attention Is All You Need (Google Brain)\n",
    "\n",
    "Weight tying between the embeddings and softmax layer:\n",
    "\n",
    "Using the Output Embedding to Improve Language Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "baeed7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11294fd8290>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a7c811a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # Default GPT-2 hyperparameters\n",
    "    context_length: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "        \n",
    "class Text_Handler:\n",
    "    \n",
    "    def __init__(self, file_name, context_length):\n",
    "        self.context_length = context_length \n",
    "        assert self.context_length != 0\n",
    "        self.raw_text = self.load_text(file_name)\n",
    "        self.vocab_size, self.stoi, self.itos = self.pre_process_text()\n",
    "        self.encode = lambda s: [self.stoi[c] for c in s]\n",
    "        self.decode = lambda l: \"\".join([self.itos[i] for i in l])\n",
    "        \n",
    "        # Load, split without shuffle and make context length exmaples.\n",
    "        self.text = torch.tensor(self.encode(self.raw_text), dtype=torch.long)\n",
    "        self.train_text, self.test_text = train_test_split(self.text, test_size=0.2, shuffle=False)\n",
    "        self.X_train, self.Y_train = self.make_examples(self.train_text)\n",
    "        self.X_test, self.Y_test = self.make_examples(self.test_text)\n",
    "                \n",
    "        \n",
    "    def load_text(self, file_name): \n",
    "        try:\n",
    "            with open(file_name, \"r\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File {file_name} does not exist within the working directory.\")\n",
    "            \n",
    "    def pre_process_text(self):\n",
    "        chars = sorted(list(set(self.raw_text)))\n",
    "        vocab_size = len(chars)\n",
    "        stoi = {char: i for i, char in enumerate(chars)}\n",
    "        itos = {i: char for char, i in stoi.items()}\n",
    "        return vocab_size, stoi, itos\n",
    "    \n",
    "    def make_examples(self, text: torch.Tensor, verbose=False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        n_examples = text.shape[0] - self.context_length\n",
    "        X = torch.empty(n_examples, self.context_length, dtype=torch.long)\n",
    "        Y = torch.empty(n_examples, dtype=torch.long)\n",
    "\n",
    "        for i in range(n_examples):\n",
    "            X[i] = text[i: i + self.context_length]\n",
    "            Y[i] = text[i + self.context_length]\n",
    "            if verbose: print(f\"Example {i+1:2d}: {X[i].tolist()} --> {Y[i].item()}\")\n",
    "\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "dd97531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 implementation\n",
    "\n",
    "class Causal_Self_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Make sure we can project the embeddings across the attention heads:\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # Layer to generate keys, queries, values in one pass:\n",
    "        self.attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Make sure the causal mask is saved to the state dict and is moved with the model to the GPU. \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.context_length, config.context_length))\n",
    "                                    .view(1, 1, config.context_length, config.context_length))\n",
    "        \n",
    "        # Final lienar transfromation after multi-head attention\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.proj_dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # batch size = number of graphs processed at once \n",
    "        # context legnth = number of nodes in the graph\n",
    "        # channels = embedding dimensionality of each node = attention_component_size\n",
    "        n_graphs, n_nodes, attention_component_size = x.size() \n",
    "        \n",
    "        # Calculate the keys, queries and values of all the nodes in each graph using the nodes' embeddings.\n",
    "        # Each component q,k,v remains the same size as the embedding dimensionality: n_embd = size_components q,k,v.\n",
    "        queries, keys, values  = self.attn(x).split(self.n_embd, dim=2) # Each have: (n_graphs, n_nodes, size_components=n_embd)\n",
    "        \n",
    "        # Split each node and its attention components across the heads (= project the The \"key\", \"query\", and \"value\"s across the heads):\n",
    "    \n",
    "        # (n_graphs, n_nodes, size_components) -> (n_graphs, n_nodes, n_heads, size_components/n_heads = split_information)^T(1,2) -> (n_graphs, n_heads, n_nodes, split_information)\n",
    "        queries = queries.view(n_graphs, n_nodes, self.n_head, attention_component_size // self.n_head).transpose(1, 2)\n",
    "        keys = keys.view(n_graphs, n_nodes, self.n_head, attention_component_size // self.n_head).transpose(1, 2)\n",
    "        values = values.view(n_graphs, n_nodes, self.n_head, attention_component_size // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        \n",
    "        # Implement scaled dot product attention for graph in each attention head. \n",
    "        # i.e. how all the nodes talk to eachother when their info is split across the heads: (attention_component_size // self.n_head). \n",
    "        # This \"split_information\" is the dimension which is dot producted over in each graph, leaving: \n",
    "        # --> n_heads * n_nodes * n_nodes ! i.e. in each head you have n_nodes talking to n_nodes. \n",
    "        \n",
    "    \n",
    "        # For each graph in each head: (n_nodes, split_information) * (n_nodes, split_information)^T\n",
    "        # (n_graphs, n_heads, n_nodes, split_information) * (n_graphs, n_heads, split_information, n_nodes) -> (n_graphs, n_heads, n_nodes, n_nodes)\n",
    "        # Normalise by the size of the split_information.\n",
    "        att = (queries @ keys.transpose(-2, -1)) * (1.0 / math.sqrt(keys.size(-1)))\n",
    "        \n",
    "        \n",
    "        # Apply the causal mask to each graph.  \n",
    "        # The indexing here allows us to handle sequences shorter than the full context length. \n",
    "        att = att.masked_fill(self.causal_mask[:,:,:n_nodes,:n_nodes] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax across how each node talks to the nodes causally connected to it.\n",
    "        # For each node this leaves a \"fraction of my attention I will pay to each other nodes' value vector\" for each node in the graph.\n",
    "        attention_fractions = self.attn_dropout(F.softmax(att, dim=-1))\n",
    "        \n",
    "        attention_weighted_value_vectors = attention_fractions @ values\n",
    "        \n",
    "        # Recombine all the heads' attention_weighted_value_vectors.\n",
    "        # Ensure this is in contigious memory.\n",
    "        attention_weighted_value_vectors = attention_weighted_value_vectors.transpose(1, 2).contiguous().view(n_graphs, n_nodes, attention_component_size)\n",
    "        \n",
    "        # Apply the final linear transforamtion to leave the output with the same dimensions as the input for a residual connection.\n",
    "        out = self.proj_dropout(self.proj(attention_weighted_value_vectors))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Layer_Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 4X step up as in the paper, and GeLu to stop dead neurons.\n",
    "        self.fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.nonlin = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.nonlin(x)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Normalise before and after the self attention as in the paper. \n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = Causal_Self_Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = Multi_Layer_Perceptron(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __inti__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embd = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            positional_embd = nn.Embedding(config.n_embd, config.n_embd),\n",
    "            attenetion_stack = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln = nn.LayerNorm(config.n_embd)))\n",
    "        \n",
    "        output_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # The output head and the token embeddings have the same dimension.\n",
    "        # So both learn vector representaions of tokens.\n",
    "        # Paper: \"We study the topmost weight matrix of neural network language models. \n",
    "        # We show that this matrix constitutes a validword embedding.\"\n",
    "        # PyTorch accumululates (adds) the gradients of both together in a backwards pass.\n",
    "        self.transformer.positional_embd.weight = self.output_head.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # Print the number of parameters\n",
    "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # \"Since layernorm [2] is used extensively throughout the model, a simple weight initialization of \n",
    "        # N(0, 0.02) was sufficient.\"\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(moudle.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                \n",
    "    def forward(self, t):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "463e93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 5 # or number of nodes in the communication graph \n",
    "d = Text_Handler(\"input_shakespeare.txt\", context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9c070504",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    context_length = context_length,\n",
    "    vocab_size = d.vocab_size,\n",
    "    n_layer = 4,\n",
    "    n_head = 3,\n",
    "    n_embd = 6,\n",
    "    bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7e49b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "c  = Causal_Self_Attention(config)\n",
    "n_batch = 4\n",
    "B, T, C = n_batch, context_length, config.n_embd # batch, time (context legnth), channel (embedding size)\n",
    "x = torch.randn(B,T,C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
